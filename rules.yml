global:
  resolve_timeout: 5m
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alertmanager@your-domain.com'
  # Optional: Add your time zone
  # smtp_hello: 'localhost'

# =====================================
# ROUTING CONFIGURATION
# =====================================
route:
  group_by: ['alertname', 'instance', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 3h
  receiver: 'default-receiver'
  
  # Route different severity levels to different receivers
  routes:
    # Critical alerts go to multiple channels immediately
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 10s
      repeat_interval: 1h
    
    # Warning alerts go to standard channel
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_wait: 2m
      repeat_interval: 6h
    
    # Info alerts (like recoveries) go to a quieter channel
    - match:
        severity: info
      receiver: 'info-alerts'
      group_wait: 5m
      repeat_interval: 24h
    
    # Device-specific routing (if you want different handling)
    - match:
        alert_type: connectivity
      receiver: 'network-team'
      group_wait: 30s

# =====================================
# NOTIFICATION RECEIVERS
# =====================================
receivers:
  # Default receiver
  - name: 'default-receiver'
    webhook_configs:
      - url: 'https://example.com/your-webhook-endpoint'
        send_resolved: true
        http_config:
          bearer_token: 'your-auth-token-here'  # if needed
        title: 'NMV Alert: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        text: |
          {{ range .Alerts }}
          **Alert:** {{ .Annotations.summary }}
          **Description:** {{ .Annotations.description }}
          **Severity:** {{ .Labels.severity }}
          **Instance:** {{ .Labels.instance }}
          **Time:** {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}

  # Critical alerts - multiple notification methods
  - name: 'critical-alerts'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXXXX/XXXXX/XXXXX'
        channel: '#critical-alerts'
        send_resolved: true
        username: 'NMV-AlertManager'
        icon_emoji: ':rotating_light:'
        title: ' CRITICAL: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        text: |
          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          
          *Details:*
          • Severity: {{ .Labels.severity }}
          • Instance: {{ .Labels.instance }}
          • Alert Type: {{ .Labels.alert_type }}
          • Started: {{ .StartsAt.Format "15:04:05" }}
          {{ if .Labels.device_name }}• Device: {{ .Labels.device_name }}{{ end }}
          {{ if .Labels.location }}• Location: {{ .Labels.location }}{{ end }}
          {{ end }}
        actions:
          - type: button
            text: 'View Grafana'
            url: 'http://localhost:3000'
          - type: button
            text: 'View Prometheus'
            url: 'http://localhost:9090'
    
    # Also send critical alerts via webhook for immediate action
    webhook_configs:
      - url: 'https://example.com/critical-webhook-endpoint'
        send_resolved: true
        title: 'CRITICAL NMV Alert'
        text: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Severity: {{ .Labels.severity }}
          Instance: {{ .Labels.instance }}
          Started: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}

  # Warning alerts
  - name: 'warning-alerts'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXXXX/XXXXX/XXXXX'
        channel: '#monitoring-warnings'
        send_resolved: true
        username: 'NMV-AlertManager'
        icon_emoji: ':warning:'
        title: ' WARNING: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        text: |
          {{ range .Alerts }}
          {{ .Annotations.summary }}
          {{ .Annotations.description }}
          
          Instance: {{ .Labels.instance }} | Started: {{ .StartsAt.Format "15:04:05" }}
          {{ end }}

  # Info alerts (recoveries, etc.)
  - name: 'info-alerts'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXXXX/XXXXX/XXXXX'
        channel: '#monitoring-info'
        send_resolved: true
        username: 'NMV-AlertManager'
        icon_emoji: ':information_source:'
        title: ' INFO: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        text: |
          {{ range .Alerts }}
          {{ .Annotations.summary }}
          {{ .Annotations.description }}
          {{ end }}

  # Network team specific alerts
  - name: 'network-team'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXXXX/XXXXX/XXXXX'
        channel: '#network-team'
        send_resolved: true
        username: 'NMV-Network-Monitor'
        icon_emoji: ':globe_with_meridians:'
        title: ' Network Alert: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        text: |
          {{ range .Alerts }}
          *Network Issue Detected:*
          {{ .Annotations.summary }}
          
          {{ .Annotations.description }}
          
          *Network Details:*
          • Instance: {{ .Labels.instance }}
          {{ if .Labels.device_name }}• Device: {{ .Labels.device_name }}{{ end }}
          {{ if .Labels.location }}• Location: {{ .Labels.location }}{{ end }}
          {{ if .Labels.vendor }}• Vendor: {{ .Labels.vendor }}{{ end }}
          • Alert Type: {{ .Labels.alert_type }}
          {{ end }}

# =====================================
# INHIBITION RULES
# =====================================
inhibit_rules:
  # If a device is completely down, don't alert on its performance metrics
  - source_match:
      alertname: DeviceDown
    target_match_re:
      alertname: High.*|Interface.*
    equal: ['instance']

  # If an interface is down, don't alert on its low throughput
  - source_match:
      alertname: InterfaceDown
    target_match:
      alertname: InterfaceThroughputDrop
    equal: ['instance', 'ifDescr']

# =====================================
# TEMPLATES (Optional - for custom formatting)
# =====================================
templates:
  - '/etc/alertmanager/templates/*.tmpl'

